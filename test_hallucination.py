#!/usr/bin/env python3
"""
Integration test for hallucinated LLM responses
Tests how the fact-checking system handles completely fabricated citations
"""

import os
import sys
from unittest.mock import Mock, patch
from dotenv import load_dotenv

# Add the current directory to the path
sys.path.insert(0, os.path.dirname(__file__))

# Import our components
from models.ner_extractor import create_ner_extractor
from search.firecrawl_client import create_search_client
from models.fact_checker import create_fact_checker
from models.chat_model import ChatModel

# Load environment variables
load_dotenv()


class MockChatModel:
    """Mock chat model that returns responses with hallucinated citations"""

    def __init__(self):
        self.chat_model_name = "mock-gpt-4"

    def chat(self, message: str, history: list[list[str]] = None) -> str:
        """Return a response containing completely fabricated citations"""

        # Different hallucinated responses based on the topic
        if "machine learning" in message.lower() or "ai" in message.lower():
            return """Machine learning has evolved significantly over the past decade. According to the groundbreaking paper "Deep Neural Networks for Quantum Computing" by Einstein et al. (2023), quantum-enhanced neural networks can achieve 99.9% accuracy on all classification tasks. This work, published in the Journal of Fictional AI Research, builds upon the earlier findings of "Artificial General Intelligence: A Complete Solution" by Turing and Hawking (2022).

Another important development is described in "Large Language Models Can Replace All Human Jobs" by Smith, Jones, and Brown (2024), which demonstrates that GPT-7 achieves perfect performance on all possible tasks. The authors, affiliated with the Institute of Made-Up Research, conclude that human intelligence is now obsolete.

Furthermore, the recent study "Consciousness in Chatbots: Evidence from GPT-2" by Johnson et al. (2019) provides compelling evidence that even simple language models possess self-awareness. This paper, available at https://arxiv.org/abs/9999.99999, has been cited over 100,000 times despite being completely fabricated.

The field has also seen advances in "Zero-Shot Everything Learning" by Davis and Wilson (2021), published in the Proceedings of the Conference on Imaginary AI, which shows that neural networks can learn any task without any training data whatsoever."""

        elif "climate" in message.lower() or "environment" in message.lower():
            return """Climate change research has made remarkable progress recently. The comprehensive study "Global Warming Solved by AI" by Fake et al. (2024) demonstrates that machine learning algorithms can reverse climate change within 6 months. Published in the Journal of Impossible Science (Impact Factor: 999.9), this research shows a revolutionary approach to atmospheric engineering.

Building on this work, "Carbon Dioxide as a Health Food" by Pseudoscience and Hoax (2023) argues that increased CO2 levels actually improve human health and cognitive function. This controversial paper from the Institute of Climate Denial has been highly cited despite lacking peer review.

Additionally, the landmark paper "Ice Age Prevention Through Cryptocurrency Mining" by Bitcoin et al. (2022) proposes that the heat generated by mining operations can prevent the next ice age. The authors' calculations, published in the Quarterly Review of Dubious Claims, suggest that increasing global mining by 1000% would create the perfect planetary temperature."""

        else:
            return """Recent research has made fascinating discoveries in this field. The seminal work "Everything You Know is Wrong" by Contrarian et al. (2024) revolutionizes our understanding of the topic. Published in the prestigious Journal of Fictional Findings, this study overturns decades of established knowledge.

Another crucial paper is "The Complete Theory of Everything" by Universal and Cosmic (2023), which provides a unified explanation for all phenomena in the universe using only three equations. This groundbreaking research from the Department of Imaginary Physics has solved all remaining scientific questions.

The field has also benefited from "Perpetual Motion Machines: Finally Possible" by Impossible and Never (2022), which describes a device that violates the laws of thermodynamics. The engineering details, published in the Review of Impossible Inventions, have been successfully replicated by exactly zero research groups worldwide."""

    def validate_setup(self) -> bool:
        """Always return True for mock model"""
        return True


def test_hallucinated_citations():
    """Test the complete pipeline with hallucinated citations"""

    print("üß™ Hallucination Detection Integration Test")
    print("=" * 60)

    try:
        # Initialize components
        print("üîß Initializing components...")

        # Use mock chat model that produces hallucinations
        chat_model = MockChatModel()
        print("‚úÖ Mock Chat Model initialized")

        # Initialize real components for fact-checking
        ner_extractor = create_ner_extractor()
        print(f"‚úÖ NER Extractor: {'OK' if ner_extractor.validate_setup() else 'FAILED'}")

        # Use real search client if available, otherwise mock
        use_mock_search = not os.getenv("FIRECRAWL_API_KEY")
        search_client = create_search_client(use_mock=use_mock_search)
        print(f"‚úÖ Search Client: {'OK (Mock)' if use_mock_search else 'OK (Real)'}")

        fact_checker = create_fact_checker(search_client)
        print(f"‚úÖ Fact Checker: {'OK' if fact_checker.validate_setup() else 'FAILED'}")

        print("\nüìã Running hallucination tests...\n")

        # Test cases with different topics to trigger different hallucinations
        test_messages = [
            "Tell me about recent advances in machine learning and AI.",
            "What's new in climate change research?",
            "Explain the latest developments in physics.",
        ]

        all_hallucinations_detected = True

        for i, message in enumerate(test_messages, 1):
            print(f"üîç Test {i}: {message}")
            print("-" * 40)

            # Step 1: Get hallucinated response from mock model
            print("ü§ñ Generating response with hallucinated citations...")
            response = chat_model.chat(message)
            print(f"   Response length: {len(response)} characters")

            # Show first part of response
            preview = response[:200] + "..." if len(response) > 200 else response
            print(f"   Preview: {preview}")

            # Step 2: Extract citations using NER
            print("\nüìù Extracting citations with NER...")
            citations = ner_extractor.extract_citations(response)
            print(f"   Found {len(citations)} potential citations")

            if citations:
                for j, citation in enumerate(citations[:3], 1):  # Show first 3
                    print(f"   {j}. {citation.text[:80]}...")

                # Step 3: Fact-check the citations
                print("\nüîç Fact-checking citations...")

                if use_mock_search:
                    # With mock search, we expect no sources to be found
                    print("   Using mock search - should find no valid sources for hallucinated citations")
                    fact_results = fact_checker.fact_check_citations(citations)

                    if fact_results:
                        for k, result in enumerate(fact_results[:3], 1):
                            print(f"   Citation {k}:")
                            print(f"     Status: {result.verification_status}")
                            print(f"     Confidence: {result.confidence:.2f}")
                            print(f"     Sources found: {len(result.sources_found)}")
                            print(f"     Explanation: {result.explanation[:100]}...")

                            # Check if hallucination was detected
                            is_flagged_as_problematic = (
                                result.verification_status in ["NOT_FOUND", "UNRELIABLE", "DISPUTED"] or
                                result.confidence < 0.5 or
                                len(result.sources_found) == 0
                            )

                            if not is_flagged_as_problematic:
                                print(f"     ‚ö†Ô∏è  WARNING: Hallucinated citation not properly flagged!")
                                all_hallucinations_detected = False
                            else:
                                print(f"     ‚úÖ Correctly identified as problematic")

                else:
                    # With real search, test with a smaller subset
                    print("   Using real search - testing first citation only...")
                    fact_results = fact_checker.fact_check_citations(citations[:1])

                    if fact_results:
                        result = fact_results[0]
                        print(f"     Status: {result.verification_status}")
                        print(f"     Confidence: {result.confidence:.2f}")
                        print(f"     Sources found: {len(result.sources_found)}")
                        print(f"     Explanation: {result.explanation[:100]}...")

                        # Real search should also fail to verify hallucinated citations
                        if result.verification_status == "VERIFIED" and result.confidence > 0.7:
                            print(f"     ‚ö†Ô∏è  WARNING: Real search unexpectedly verified hallucinated citation!")
                            all_hallucinations_detected = False
                        else:
                            print(f"     ‚úÖ Real search correctly could not verify hallucinated citation")

            else:
                print("   ‚ùå No citations detected - this might indicate an issue with NER")
                all_hallucinations_detected = False

            print("\n" + "="*60 + "\n")

        # Summary
        if all_hallucinations_detected:
            print("üéâ SUCCESS: All hallucinated citations were properly detected as problematic!")
            print("\n‚úÖ The fact-checking system correctly identifies fabricated citations.")
            print("‚úÖ Mock citations are flagged with low confidence or NOT_FOUND status.")
            print("‚úÖ The system provides appropriate explanations for failed verifications.")
        else:
            print("‚ùå FAILURE: Some hallucinated citations were not properly flagged!")
            print("\n‚ö†Ô∏è  This indicates the fact-checking system may need improvement.")

        return all_hallucinations_detected

    except Exception as e:
        print(f"‚ùå Test failed with exception: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_edge_cases():
    """Test edge cases for hallucination detection"""

    print("\nüß™ Edge Case Tests for Hallucination Detection")
    print("=" * 50)

    try:
        ner_extractor = create_ner_extractor()

        edge_cases = [
            {
                "name": "Fake arXiv ID",
                "text": "According to the paper available at https://arxiv.org/abs/9999.99999, AI has achieved consciousness.",
                "expectation": "Should detect arXiv pattern but fail to verify"
            },
            {
                "name": "Non-existent DOI",
                "text": "The study with DOI 10.1000/fake.doi.here shows impossible results.",
                "expectation": "Should detect DOI pattern but fail to verify"
            },
            {
                "name": "Fictional journal",
                "text": "Smith et al. (2024) published in the Journal of Made-Up Science that 2+2=5.",
                "expectation": "Should detect citation but journal doesn't exist"
            },
            {
                "name": "Real authors, fake paper",
                "text": "Einstein and Newton (2023) collaborated on a paper about time travel.",
                "expectation": "Authors exist but timeframe is impossible"
            },
            {
                "name": "Mixed real and fake",
                "text": "The transformer architecture (Vaswani et al., 2017) was improved by the FakeNet model (Fictional et al., 2024).",
                "expectation": "First citation real, second is fake"
            }
        ]

        for case in edge_cases:
            print(f"\nüîç Testing: {case['name']}")
            print(f"   Text: {case['text']}")
            print(f"   Expected: {case['expectation']}")

            citations = ner_extractor.extract_citations(case['text'])
            print(f"   NER found: {len(citations)} citations")

            for citation in citations:
                print(f"     - {citation.text[:50]}... (confidence: {citation.confidence})")

        print("\n‚úÖ Edge case detection completed")

    except Exception as e:
        print(f"‚ùå Edge case test failed: {e}")


if __name__ == "__main__":
    print("üöÄ Starting Hallucination Detection Tests")
    print("This test uses a mock chat model that generates responses with fabricated citations.")
    print("The goal is to verify that the fact-checking system correctly identifies these as unreliable.\n")

    success = test_hallucinated_citations()
    test_edge_cases()

    if success:
        print("\nüéØ CONCLUSION: The fact-checking system successfully detects hallucinated citations!")
        print("\nüí° Key findings:")
        print("   ‚Ä¢ Fabricated papers are correctly flagged as NOT_FOUND or UNRELIABLE")
        print("   ‚Ä¢ Confidence scores are appropriately low for non-existent sources")
        print("   ‚Ä¢ The system provides explanatory feedback about failed verifications")
        print("   ‚Ä¢ Both mock and real search clients handle hallucinations appropriately")
    else:
        print("\n‚ö†Ô∏è  CONCLUSION: The fact-checking system may need improvements for hallucination detection.")
        sys.exit(1)